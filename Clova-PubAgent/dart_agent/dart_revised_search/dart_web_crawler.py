"""
DART ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ìœ¼ë¡œ ì •ì •ê³µì‹œ ì°¾ê¸°
https://dart.fss.or.kr/dsab001/searchCorp.ax ë¥¼ ì´ìš©í•œ ë°ì´í„° ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import time

class DartWebCrawler:
    """DART ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë„êµ¬"""

    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://dart.fss.or.kr"
        self.search_url = "https://dart.fss.or.kr/dsab001/searchCorp.ax"

        # í—¤ë” ì„¤ì • (ì›¹ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Whale/4.33.325.17 Safari/537.36',
            'Accept': 'text/html, */*; q=0.01',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'X-Requested-With': 'XMLHttpRequest',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Referer': 'https://dart.fss.or.kr/dsab001/search.ax',
        })

    def search_company_disclosures(self, company_name, start_date=None, end_date=None, days=365):
        """íšŒì‚¬ëª…ìœ¼ë¡œ ê³µì‹œ ê²€ìƒ‰"""

        # ë‚ ì§œ ì„¤ì •
        if end_date is None:
            end_date = datetime.now()
        elif isinstance(end_date, str):
            end_date = datetime.strptime(end_date, '%Y-%m-%d')

        if start_date is None:
            start_date = end_date - timedelta(days=days)
        elif isinstance(start_date, str):
            start_date = datetime.strptime(start_date, '%Y-%m-%d')

        print(f"ğŸ” '{company_name}' ê³µì‹œ ê²€ìƒ‰ ì¤‘...")
        print(f"ğŸ“… ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

        # POST ë°ì´í„° ì„¤ì • (ì‹¤ì œ DART ì‚¬ì´íŠ¸ íŒŒë¼ë¯¸í„°)
        form_data = {
            'currentPage': '1',
            'maxResults': '50',
            'maxLinks': '10',
            'sort': 'date',
            'series': 'desc',
            'textCrpNm': company_name,  # íšŒì‚¬ëª…
            'startDate': start_date.strftime('%Y%m%d'),  # YYYYMMDD í˜•ì‹
            'endDate': end_date.strftime('%Y%m%d'),      # YYYYMMDD í˜•ì‹
            'pageGubun': 'corp',
            'decadeType': 'finalReport',
            'recent': '',
            'attachDocNm': ''
        }

        try:
            response = self.session.post(self.search_url, data=form_data)
            response.raise_for_status()
            # response ê²°ê³¼
            print(f"âœ… ìš”ì²­ ì„±ê³µ: ìƒíƒœ ì½”ë“œ {response.status_code}")

            return self.parse_search_results(response.text)

        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    def parse_search_results(self, html_content):
        """HTML íŒŒì‹±í•˜ì—¬ ê³µì‹œ ëª©ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')

        disclosures = []

        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table', class_='tbList')
        if not table:
            print("âŒ ê³µì‹œ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ë°ì´í„° í–‰ë“¤ ì°¾ê¸°
        rows = table.find('tbody').find_all('tr')

        for row in rows:            
            try:
                cols = row.find_all('td')
                if len(cols) < 5:
                    continue

                # ë°ì´í„° ì¶”ì¶œ
                number = cols[0].get_text(strip=True)
                company_cell = cols[1]
                report_cell = cols[2]
                submitter = cols[3].get_text(strip=True)
                date = cols[4].get_text(strip=True)

                # íšŒì‚¬ëª… ì¶”ì¶œ
                company_link = company_cell.find('a')
                company_name = company_link.get_text(strip=True) if company_link else ""

                # ë³´ê³ ì„œëª…ê³¼ ë§í¬ ì¶”ì¶œ
                report_link = report_cell.find('a')
                if report_link:
                    report_name = report_link.get_text(strip=True)
                    report_url = report_link.get('href', '')

                    # rcpNo ì¶”ì¶œ
                    rcept_no_match = re.search(r'rcpNo=(\d+)', report_url)
                    rcept_no = rcept_no_match.group(1) if rcept_no_match else ""
                else:
                    report_name = report_cell.get_text(strip=True)
                    report_url = ""
                    rcept_no = ""

                # ì •ì •ê³µì‹œì¸ì§€ í™•ì¸
                correction_keywords = ['[ê¸°ì¬ì •ì •]', '[ì²¨ë¶€ì •ì •]', '[ì²¨ë¶€ì¶”ê°€]', '[ì •ì •ëª…ë ¹ë¶€ê³¼]', '[ì •ì •ì œì¶œìš”êµ¬]', 'ì •ì •', 'í’ë¬¸']
                is_correction = any(keyword in report_name for keyword in correction_keywords)
                
                time.sleep(0.2)
                if is_correction == 'false' or is_correction == False or is_correction == 'False':
                    continue

                disclosure = {
                    'number': number,
                    'company': company_name,
                    'report': report_name,
                    'submitter': submitter,
                    'date': date,
                    'rcept_no': rcept_no,
                    'url': f"https://dart.fss.or.kr{report_url}" if report_url else "",
                    'is_correction': is_correction
                }

                disclosures.append(disclosure)

            except Exception as e:
                print(f"âš ï¸ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        return disclosures

    def find_corrections(self, company_name, start_date=None, end_date=None, days=365):
        """ì •ì •ê³µì‹œë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜"""
        all_disclosures = self.search_company_disclosures(company_name, start_date, end_date, days)

        corrections = [d for d in all_disclosures if d['is_correction']]

        print(f"ğŸ“Š ì „ì²´ ê³µì‹œ: {len(all_disclosures)}ê±´")
        print(f"ğŸ”§ ì •ì •ê³µì‹œ: {len(corrections)}ê±´")

        return corrections

    def print_results(self, corrections):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\nâœ… {len(corrections)}ê±´ì˜ ì •ì •ê³µì‹œ ë°œê²¬!")
        print("=" * 80)

        if not corrections:
            print("ì •ì •ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i, item in enumerate(corrections, 1):
            print(f"{i}. ğŸ“‹ {item['company']}")
            print(f"   ë³´ê³ ì„œ: {item['report']}")
            print(f"   ì œì¶œì¸: {item['submitter']}")
            print(f"   ë‚ ì§œ: {item['date']}")
            if item['url']:
                print(f"   ğŸ”— ë§í¬: {item['url']}")
            print("-" * 80)


if __name__ == "__main__":
    crawler = DartWebCrawler()

    print("ğŸ¯ DART ì›¹ í¬ë¡¤ë§ ì •ì •ê³µì‹œ ì°¾ê¸°")
    print("=" * 35)

    while True:
        company = input("\nğŸ¢ íšŒì‚¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()

        if company.lower() == 'q':
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not company:
            print("âŒ íšŒì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        # ë‚ ì§œ ì…ë ¥ (ì„ íƒì‚¬í•­)
        print("\nğŸ“… ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • (ì„ íƒì‚¬í•­):")
        start_input = input("ì‹œì‘ì¼ (YYYY-MM-DD ë˜ëŠ” ì—”í„°ë¡œ ê¸°ë³¸ê°’): ").strip()
        end_input = input("ì¢…ë£Œì¼ (YYYY-MM-DD ë˜ëŠ” ì—”í„°ë¡œ ì˜¤ëŠ˜): ").strip()

        start_date = start_input if start_input else None
        end_date = end_input if end_input else None

        if not start_date and not end_date:
            days_input = input("ìµœê·¼ ë©°ì¹ ê°„ ê²€ìƒ‰? (ê¸°ë³¸ê°’: 365ì¼): ").strip()
            days = int(days_input) if days_input.isdigit() else 365
        else:
            days = 365

        # ê²€ìƒ‰ ì‹¤í–‰
        try:
            corrections = crawler.find_corrections(company, start_date, end_date, days)
            crawler.print_results(corrections)
        except ValueError as e:
            print(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
            print("ì˜¬ë°”ë¥¸ í˜•ì‹: YYYY-MM-DD (ì˜ˆ: 2025-01-01)")