"""
DART ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
ë§í¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì‹¤ì œ ê³µì‹œë¬¸ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, parse_qs, urlparse


class DartDocumentFetcher:
    """DART ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ë„êµ¬"""

    def __init__(self):
        self.base_url = "https://dart.fss.or.kr"
        self.session = requests.Session()

        # í—¤ë” ì„¤ì • (ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ í—¤ë” ì‚¬ìš©)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Whale/4.33.325.17 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,es;q=0.6',
            'sec-ch-ua': '"Chromium";v="138", "Whale";v="4", "Not.A/Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'document',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-site': 'same-origin',
            'upgrade-insecure-requests': '1'
        })

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):  # noqa: U100
        self.session.close()

    def extract_rcept_no(self, dart_url):
        """DART URLì—ì„œ ì ‘ìˆ˜ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            parsed_url = urlparse(dart_url)
            query_params = parse_qs(parsed_url.query)
            rcept_no = query_params.get('rcpNo', [None])[0]
            return rcept_no
        except Exception as e:
            print(f"âŒ ì ‘ìˆ˜ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_dcm_no(self, soup):
        """HTMLì—ì„œ dcmNo ì¶”ì¶œ"""
        try:
            # iframe srcì—ì„œ dcmNo ì°¾ê¸°
            iframes = soup.find_all('iframe')
            for iframe in iframes:
                src = iframe.get('src', '')
                if 'dcmNo=' in src:
                    try:
                        dcm_no = src.split('dcmNo=')[1].split('&')[0]
                        print(f"ğŸ” iframeì—ì„œ ë°œê²¬ëœ dcmNo: {dcm_no}")
                        return dcm_no
                    except:
                        print("âš ï¸ iframeì—ì„œ dcmNo ì¶”ì¶œ ì‹¤íŒ¨")
                        pass

            # ìŠ¤í¬ë¦½íŠ¸ì—ì„œ dcmNo ì°¾ê¸° - ë” ë„“ì€ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
            scripts = soup.find_all('script')
            for script in scripts:
                script_text = script.get_text()
                if 'dcmNo' in script_text:
                    import re
                    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
                    patterns = [
                        r'viewDoc\([^,]+,\s*["\']?(\d{7,})["\']?',  # viewDoc í•¨ìˆ˜ í˜¸ì¶œì—ì„œ dcmNo ì¶”ì¶œ
                        r'linkDoc\([^,]+,\s*["\']?(\d{7,})["\']?',  # linkDoc í•¨ìˆ˜ í˜¸ì¶œì—ì„œ dcmNo ì¶”ì¶œ
                        r'dcmNo["\']?\s*[:=]\s*["\']?(\d+)',
                        r'"dcmNo"\s*:\s*"(\d+)"',
                        r"'dcmNo'\s*:\s*'(\d+)'",
                        r'dcmNo\s*=\s*"(\d+)"',
                        r'dcmNo\s*=\s*\'(\d+)\'',
                        r'dcmNo\s*=\s*(\d+)',
                        r'dcmNo":"(\d+)"',
                        r"dcmNo':'(\d+)'",
                        r"dcmNo\]\s*=\s*[\"'](\d+)[\"']",  # node1['dcmNo'] = "10835040"
                        r"dcmNo['\"]?\]\s*=\s*[\"'](\d+)[\"']"
                    ]

                    for pattern in patterns:
                        match = re.search(pattern, script_text)
                        if match:
                            dcm_no = match.group(1)
                            print(f"ğŸ” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°œê²¬ëœ dcmNo: {dcm_no} (íŒ¨í„´: {pattern})")
                            return dcm_no

            # URL íŒ¨í„´ì—ì„œ dcmNo ì°¾ê¸°
            links = soup.find_all('a')
            for link in links:
                href = link.get('href', '')
                if 'dcmNo=' in href:
                    try:
                        dcm_no = href.split('dcmNo=')[1].split('&')[0]
                        if dcm_no and dcm_no.isdigit():
                            print(f"ğŸ” ë§í¬ì—ì„œ ë°œê²¬ëœ dcmNo: {dcm_no}")
                            return dcm_no
                    except:
                        pass

            # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ dcmNo íŒ¨í„´ ì°¾ê¸°
            all_text = soup.get_text()
            if 'dcmNo' in all_text:
                import re
                match = re.search(r'dcmNo["\']?\s*[:=]\s*["\']?(\d+)', all_text)
                if match:
                    dcm_no = match.group(1)
                    print(f"ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ dcmNo: {dcm_no}")
                    return dcm_no

            # ê¸°ë³¸ê°’ ì‚¬ìš©
            dcm_no = ""
            print(f"ğŸ” dcmNoë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹ˆ ê°’ ì‚¬ìš©")
            return dcm_no

        except Exception as e:
            print(f"âš ï¸ dcmNo ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    def get_document_content(self, dart_url):
        """DART ë§í¬ì—ì„œ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        print(f"ğŸ” ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        print(f"ğŸ”— URL: {dart_url}")

        rcept_no = self.extract_rcept_no(dart_url)
        if not rcept_no:
            print("âŒ ì ‘ìˆ˜ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print(f"ğŸ“‹ ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no}")

        try:
            # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼í•˜ì—¬ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            print(f"ğŸ” ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼: {dart_url}")
            response = self.session.get(dart_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # 2. ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            document_info = self.extract_document_info(soup)

            # 3. ë©”ì¸ í˜ì´ì§€ì—ì„œ dcmNo ì°¾ê¸°
            dcm_no = self.extract_dcm_no(soup)

            # 4. HTML í˜•ì‹ ìš°ì„  ì‹œë„ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´)
            print(f"ğŸ¯ HTML í˜•ì‹ìœ¼ë¡œ ìš°ì„  ì‹œë„...")
            viewer_url = f"https://dart.fss.or.kr/report/viewer.do?rcpNo={rcept_no}&dcmNo={dcm_no}&eleId=1&offset=0&length=0&dtd=HTML"
            content = self.fetch_document_text(viewer_url, referer=dart_url)

            # HTML í˜•ì‹ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²•ë“¤ ì‹œë„
            if not content or len(content.strip()) == 0:
                print(f"ğŸ”„ HTML í˜•ì‹ ì‹¤íŒ¨, XML í˜•ì‹ë“¤ ì‹œë„...")

                # 1. ê¸°ë³¸ XML í˜•ì‹
                viewer_url_2 = f"https://dart.fss.or.kr/report/viewer.do?rcpNo={rcept_no}&dcmNo={dcm_no}&eleId=1&offset=0&length=0&dtd=dart4.xsd"
                print(f"ğŸ¯ XML í˜•ì‹ ì‹œë„: {viewer_url_2}")
                content = self.fetch_document_text(viewer_url_2, referer=dart_url)

                # 2. eleId=0ìœ¼ë¡œ ì‹œë„
                if not content or len(content.strip()) == 0:
                    viewer_url_3 = f"https://dart.fss.or.kr/report/viewer.do?rcpNo={rcept_no}&dcmNo={dcm_no}&eleId=0&offset=0&length=0&dtd=dart4.xsd"
                    print(f"ğŸ¯ eleId=0ìœ¼ë¡œ ì‹œë„: {viewer_url_3}")
                    content = self.fetch_document_text(viewer_url_3, referer=dart_url)

                # 3. dcmNo ì—†ì´ ì‹œë„
                if not content or len(content.strip()) == 0:
                    viewer_url_4 = f"https://dart.fss.or.kr/report/viewer.do?rcpNo={rcept_no}&eleId=1&offset=0&length=0&dtd=HTML"
                    print(f"ğŸ¯ dcmNo ì—†ì´ HTML ì‹œë„: {viewer_url_4}")
                    content = self.fetch_document_text(viewer_url_4, referer=dart_url)

            document_info['content'] = content

            return document_info

        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def extract_document_info(self, soup):
        """ë¬¸ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        info = {
            'title': '',
            'company': '',
            'submitter': '',
            'submit_date': '',
            'content': ''
        }

        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('title')
            if title_elem:
                info['title'] = title_elem.get_text(strip=True)

            # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)

                        if 'íšŒì‚¬ëª…' in key or 'ê³µì‹œëŒ€ìƒíšŒì‚¬' in key:
                            info['company'] = value
                        elif 'ì œì¶œì¸' in key:
                            info['submitter'] = value
                        elif 'ì ‘ìˆ˜ì¼ì' in key or 'ì œì¶œì¼ì' in key:
                            info['submit_date'] = value

        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return info

    def find_content_url(self, soup, rcept_no):
        """ë³¸ë¬¸ ë§í¬ ì°¾ê¸° - iframe srcì—ì„œ ì¶”ì¶œ"""
        try:
            # iframe ì°¾ê¸°
            iframe = soup.find('iframe', {'id': 'ifrm'})
            if iframe:
                src = iframe.get('src', '')
                if src:
                    full_url = urljoin(self.base_url, src)
                    print(f"ğŸ¯ iframeì—ì„œ ë°œê²¬ëœ ë¬¸ì„œ URL: {full_url}")
                    return full_url

            # iframeì´ ì—†ë‹¤ë©´ ë‹¤ë¥¸ iframe ì°¾ê¸°
            iframes = soup.find_all('iframe')
            for iframe in iframes:
                src = iframe.get('src', '')
                if 'viewer.do' in src or 'report' in src:
                    full_url = urljoin(self.base_url, src)
                    print(f"ğŸ¯ iframeì—ì„œ ë°œê²¬ëœ ë¬¸ì„œ URL: {full_url}")
                    return full_url

            # ë³¸ë¬¸ ë²„íŠ¼ì´ë‚˜ ë§í¬ ì°¾ê¸°
            links = soup.find_all('a')
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                # ë³¸ë¬¸ ê´€ë ¨ ë§í¬ íŒ¨í„´
                if any(keyword in text for keyword in ['ë³¸ë¬¸', 'ì „ì²´', 'ë‚´ìš©ë³´ê¸°', 'ë¬¸ì„œë³´ê¸°']):
                    if href:
                        full_url = urljoin(self.base_url, href)
                        print(f"ğŸ” ë°œê²¬ëœ ë³¸ë¬¸ ë§í¬: {full_url}")
                        return full_url

            # ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ URL ìƒì„±
            content_url = f"{self.base_url}/report/viewer.do?rcpNo={rcept_no}"
            print(f"ğŸ” ê¸°ë³¸ URL ì‚¬ìš©: {content_url}")
            return content_url

        except Exception as e:
            print(f"âš ï¸ ë³¸ë¬¸ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None

    def fetch_document_text(self, content_url, referer=None):
        """ì‹¤ì œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"ğŸ“„ ë³¸ë¬¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {content_url}")

            # Referer í—¤ë” ì„¤ì •
            headers = {}
            if referer:
                headers = {
                    'Referer': referer,
                    'sec-fetch-dest': 'iframe'
                }

            response = self.session.get(content_url, headers=headers)
            response.raise_for_status()

            print(f"ğŸ” í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            print(f"ğŸ” ì‘ë‹µ ê¸¸ì´: {len(response.content)} ë¬¸ì")

            # XML ê²½ê³  ë°©ì§€
            import warnings
            from bs4 import XMLParsedAsHTMLWarning
            warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

            # ì¸ì½”ë”© ë¬¸ì œ í•´ê²° ì‹œë„ - DART íŠ¹ìˆ˜ ì²˜ë¦¬
            html_content = None

            # DART ì‘ë‹µì´ XMLì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            content_type = response.headers.get('content-type', '').lower()
            print(f"ğŸ” Content-Type: {content_type}")

            # 1. ì¸ì½”ë”© ìë™ ê°ì§€ ì‹œë„ (ë” ì •í™•í•œ í•œê¸€ ê°ì§€)
            try:
                import chardet
                detected = chardet.detect(response.content)
                detected_encoding = detected.get('encoding')
                confidence = detected.get('confidence', 0)

                # í•œê¸€ ì»¨í…ì¸ ì— ëŒ€í•´ EUC-KR ìš°ì„  ì²˜ë¦¬
                if detected_encoding and confidence > 0.3:  # ì‹ ë¢°ë„ ë” ë‚®ì¶¤
                    # EUC-KR ê³„ì—´ ì¸ì½”ë”©ì´ ê°ì§€ë˜ë©´ ìš°ì„  ì‹œë„
                    if detected_encoding.lower() in ['euc-kr', 'cp949', 'ks_c_5601-1987']:
                        try:
                            html_content = response.content.decode('euc-kr')
                            print(f"ğŸ” EUC-KR ìš°ì„  ë””ì½”ë”© ì„±ê³µ")
                        except:
                            html_content = response.content.decode(detected_encoding)
                            print(f"ğŸ” ìë™ ê°ì§€ëœ ì¸ì½”ë”© {detected_encoding} ì‚¬ìš© (ì‹ ë¢°ë„: {confidence:.2f})")
                    else:
                        html_content = response.content.decode(detected_encoding)
                        print(f"ğŸ” ìë™ ê°ì§€ëœ ì¸ì½”ë”© {detected_encoding} ì‚¬ìš© (ì‹ ë¢°ë„: {confidence:.2f})")
                else:
                    print(f"âš ï¸ ìë™ ê°ì§€ ì‹¤íŒ¨ ë˜ëŠ” ë‚®ì€ ì‹ ë¢°ë„: {detected_encoding} ({confidence:.2f})")
            except ImportError:
                print("âš ï¸ chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ìˆ˜ë™ ì¸ì½”ë”© ì‹œë„")
            except Exception as e:
                print(f"âš ï¸ ì¸ì½”ë”© ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")

            # 2. ì‘ë‹µ í—¤ë”ì—ì„œ ì¸ì½”ë”© í™•ì¸
            if html_content is None:
                if 'charset=' in content_type:
                    charset = content_type.split('charset=')[1].split(';')[0].strip()
                    try:
                        html_content = response.content.decode(charset)
                        print(f"ğŸ” í—¤ë”ì—ì„œ ê°ì§€ëœ ì¸ì½”ë”© {charset} ì‚¬ìš©")
                    except:
                        print(f"âš ï¸ í—¤ë” ì¸ì½”ë”© {charset} ì‹¤íŒ¨")

            # 3. ìˆ˜ë™ ì¸ì½”ë”© ì‹œë„ (DARTìš© ìš°ì„ ìˆœìœ„ ìµœì í™”)
            if html_content is None:
                # DARTëŠ” ì£¼ë¡œ EUC-KRì„ ì‚¬ìš©í•˜ë¯€ë¡œ ìš°ì„ ìˆœìœ„ ì¡°ì •
                encodings = ['euc-kr', 'cp949', 'utf-8', 'iso-8859-1', 'utf-8-sig']

                for encoding in encodings:
                    try:
                        test_content = response.content.decode(encoding)

                        # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ í™•ì¸ (ê°€-í£, ã„±-ã…, ã…-ã…£)
                        import re
                        korean_pattern = re.compile(r'[ê°€-í£ã„±-ã…ã…-ã…£]')

                        if korean_pattern.search(test_content):
                            html_content = test_content
                            print(f"ğŸ” {encoding} ë””ì½”ë”© ì„±ê³µ - í•œê¸€ í™•ì¸ë¨")
                            break
                        else:
                            print(f"âš ï¸ {encoding}ìœ¼ë¡œ ë””ì½”ë”©í–ˆì§€ë§Œ í•œê¸€ ì—†ìŒ")

                    except UnicodeDecodeError:
                        print(f"âš ï¸ {encoding} ë””ì½”ë”© ì‹¤íŒ¨")
                        continue

            # 4. ìµœí›„ì˜ ìˆ˜ë‹¨
            if html_content is None:
                html_content = response.content.decode('utf-8', errors='ignore')
                print(f"ğŸ” UTF-8 ë””ì½”ë”© (ì—ëŸ¬ ë¬´ì‹œ)")

            # 5. DART XML íŠ¹ìˆ˜ ì²˜ë¦¬ - ì´ë¯¸ ë³€í™˜ëœ í•œê¸€ ë³µì› ì‹œë„
            if '?' in html_content and 'ì—«' in html_content:
                print("âš ï¸ í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ê°ì§€, ë³µì› ì‹œë„...")
                try:
                    # ì˜ëª» ì¸ì½”ë”©ëœ í•œê¸€ì„ ì˜¬ë°”ë¥´ê²Œ ë³µì›
                    html_content_bytes = html_content.encode('iso-8859-1')
                    html_content = html_content_bytes.decode('euc-kr')
                    print("ğŸ” í•œê¸€ ì¸ì½”ë”© ë³µì› ì„±ê³µ")
                except:
                    try:
                        html_content_bytes = html_content.encode('cp1252')
                        html_content = html_content_bytes.decode('euc-kr')
                        print("ğŸ” í•œê¸€ ì¸ì½”ë”© ë³µì› ì„±ê³µ (cp1252->euc-kr)")
                    except:
                        print("âš ï¸ í•œê¸€ ì¸ì½”ë”© ë³µì› ì‹¤íŒ¨")

            soup = BeautifulSoup(html_content, 'html.parser')

            # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"ğŸ” HTML ì²« 500ì: {html_content[:500]}...")

            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'meta', 'link']):
                tag.decompose()

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
            text = soup.get_text()

            # ê³µë°± ì •ë¦¬
            lines = [line.strip() for line in text.splitlines()]
            lines = [line for line in lines if line]

            cleaned_text = '\n'.join(lines)

            print(f"ğŸ” ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")

            return cleaned_text

        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def print_document(self, doc_info):
        """ë¬¸ì„œ ë‚´ìš© ì¶œë ¥"""
        if not doc_info:
            print("âŒ ë¬¸ì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "="*80)
        print("ğŸ“„ DART ë¬¸ì„œ ë‚´ìš©")
        print("="*80)

        print(f"ğŸ“‹ ì œëª©: {doc_info.get('title', 'N/A')}")
        print(f"ğŸ¢ íšŒì‚¬: {doc_info.get('company', 'N/A')}")
        print(f"ğŸ‘¤ ì œì¶œì¸: {doc_info.get('submitter', 'N/A')}")
        print(f"ğŸ“… ì œì¶œì¼: {doc_info.get('submit_date', 'N/A')}")

        print("\n" + "-"*80)
        print("ğŸ“„ ë¬¸ì„œ ë‚´ìš©:")
        print("-"*80)
        print(doc_info.get('content', 'ë‚´ìš© ì—†ìŒ'))
        print("="*80)


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_document_fetcher():
    """ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° í…ŒìŠ¤íŠ¸"""
    # í…ŒìŠ¤íŠ¸ìš© DART ë§í¬ë“¤ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë§í¬)
    test_urls = [
        "https://dart.fss.or.kr/dsaf001/main.do?rcpNo=20251001800663",  # LGí™”í•™ ì •ì •ê³µì‹œ
        "https://dart.fss.or.kr/dsaf001/main.do?rcpNo=20250828800490",  # LGí™”í•™ ì •ì •ê³µì‹œ
    ]

    with DartDocumentFetcher() as fetcher:
        for i, url in enumerate(test_urls, 1):
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {url}")
            doc_info = fetcher.get_document_content(url)
            fetcher.print_document(doc_info)
            print("\n" + "="*100)


if __name__ == "__main__":
    print("ğŸ¯ DART ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ë„êµ¬")
    print("=" * 40)

    while True:
        print("\nğŸ“‹ ì˜µì…˜ ì„ íƒ:")
        print("1. DART ë§í¬ë¡œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°")
        print("2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("3. ì¢…ë£Œ")

        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()

        if choice == '1':
            url = input("\nğŸ”— DART URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if url:
                with DartDocumentFetcher() as fetcher:
                    doc_info = fetcher.get_document_content(url)
                    fetcher.print_document(doc_info)

        elif choice == '2':
            test_document_fetcher()

        elif choice == '3':
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")