"""
DART ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ìœ¼ë¡œ ì •ì •ê³µì‹œ ì°¾ê¸°
https://dart.fss.or.kr/dsab001/searchCorp.ax ë¥¼ ì´ìš©í•œ ë°ì´í„° ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import time

class DartWebCrawler:
    """DART ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë„êµ¬"""

    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://dart.fss.or.kr"
        self.search_url = "https://dart.fss.or.kr/dsab001/searchCorp.ax"
        self.recent_url = "https://dart.fss.or.kr/dsac001/search.ax"  # ìµœê·¼ ê³µì‹œ ëª©ë¡ URL

        # í—¤ë” ì„¤ì • (ì›¹ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
            'Accept': 'text/html, */*; q=0.01',
            'Accept-Language': 'ko,en-US;q=0.9,en;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'X-Requested-With': 'XMLHttpRequest',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Referer': 'https://dart.fss.or.kr/dsac001/mainAll.do',
        })

    def get_recent_disclosures(self, mday_cnt=1, page=1, max_results=50, company_name="", fetch_all=False):
        """
        ìµœê·¼ ê³µì‹œ ì „ì²´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (https://dart.fss.or.kr/dsac001/mainAll.do)

        Args:
            mday_cnt (int): ê²€ìƒ‰ ê¸°ê°„ (1=ë‹¹ì¼, 2=2ì¼, 3=3ì¼, 7=7ì¼, 30=30ì¼)
            page (int): í˜ì´ì§€ ë²ˆí˜¸
            max_results (int): í˜ì´ì§€ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            company_name (str): íŠ¹ì • íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
            fetch_all (bool): ì „ì²´ í˜ì´ì§€ë¥¼ ë‹¤ ê°€ì ¸ì˜¬ì§€ ì—¬ë¶€ (ê¸°ë³¸ False)

        Returns:
            list: ê³µì‹œ ëª©ë¡
        """
        if fetch_all:
            return self.get_all_recent_disclosures(mday_cnt, company_name, max_results)

        print(f"ğŸ” ìµœê·¼ {mday_cnt}ì¼ê°„ ê³µì‹œ ê²€ìƒ‰ ì¤‘... (í˜ì´ì§€ {page})")

        # POST ë°ì´í„° ì„¤ì •
        form_data = {
            'currentPage': str(page),
            'maxResults': str(max_results),
            'maxLinks': '10',
            'sort': '',
            'series': '',
            'pageGrouping': '',
            'mdayCnt': str(mday_cnt),
            'selectDate': '',
            'textCrpCik': company_name  # íšŒì‚¬ëª… í•„í„°ë§ (ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´)
        }

        try:
            response = self.session.post(self.recent_url, data=form_data)
            response.raise_for_status()
            print(f"âœ… ìš”ì²­ ì„±ê³µ: ìƒíƒœ ì½”ë“œ {response.status_code}")

            return self.parse_recent_results(response.text)

        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    def get_all_recent_disclosures(self, mday_cnt=1, company_name="", max_results=100):
        """
        ìµœê·¼ ê³µì‹œ ì „ì²´ë¥¼ ëª¨ë“  í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸°

        Args:
            mday_cnt (int): ê²€ìƒ‰ ê¸°ê°„ (1=ë‹¹ì¼, 2=2ì¼, 3=3ì¼, 7=7ì¼, 30=30ì¼)
            company_name (str): íŠ¹ì • íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
            max_results (int): í˜ì´ì§€ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ 100)

        Returns:
            list: ì „ì²´ ê³µì‹œ ëª©ë¡
        """
        print(f"ğŸ” ìµœê·¼ {mday_cnt}ì¼ê°„ ê³µì‹œ ì „ì²´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘...")
        if company_name:
            print(f"ğŸ¢ íšŒì‚¬ëª… í•„í„°: {company_name}")

        all_disclosures = []
        page = 1
        max_pages = 100  # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 100í˜ì´ì§€ê¹Œì§€ë§Œ

        while page <= max_pages:
            print(f"\nğŸ“„ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

            # POST ë°ì´í„° ì„¤ì •
            form_data = {
                'currentPage': str(page),
                'maxResults': str(max_results),
                'maxLinks': '10',
                'sort': '',
                'series': '',
                'pageGrouping': '',
                'mdayCnt': str(mday_cnt),
                'selectDate': '',
                'textCrpCik': company_name
            }

            try:
                response = self.session.post(self.recent_url, data=form_data)
                response.raise_for_status()

                disclosures = self.parse_recent_results(response.text)
                print(f"âœ… í˜ì´ì§€ {page} ìš”ì²­ ì„±ê³µ: {len(disclosures)}ê±´ ë°œê²¬")

                if not disclosures or len(disclosures) == 0:
                    print(f"âœ… í˜ì´ì§€ {page}ì—ì„œ ê³µì‹œê°€ ì—†ìŒ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                all_disclosures.extend(disclosures)
                print(f"âœ… í˜ì´ì§€ {page}: {len(disclosures)}ê±´ ì¶”ê°€ (ëˆ„ì : {len(all_disclosures)}ê±´)")

                # ê²°ê³¼ê°€ max_resultsë³´ë‹¤ ì ìœ¼ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€
                if len(disclosures) < max_results:
                    print(f"âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (í˜ì´ì§€ {page})")
                    break

                page += 1
                time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
                break

        print(f"\nğŸ‰ ì „ì²´ {len(all_disclosures)}ê±´ì˜ ê³µì‹œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")
        return all_disclosures

    def search_company_disclosures(self, company_name, start_date=None, end_date=None, days=365):
        """íšŒì‚¬ëª…ìœ¼ë¡œ ê³µì‹œ ê²€ìƒ‰"""

        # ë‚ ì§œ ì„¤ì •
        if end_date is None:
            end_date = datetime.now()
        elif isinstance(end_date, str):
            end_date = datetime.strptime(end_date, '%Y-%m-%d')

        if start_date is None:
            start_date = end_date - timedelta(days=days)
        elif isinstance(start_date, str):
            start_date = datetime.strptime(start_date, '%Y-%m-%d')

        print(f"ğŸ” '{company_name}' ê³µì‹œ ê²€ìƒ‰ ì¤‘...")
        print(f"ğŸ“… ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

        # POST ë°ì´í„° ì„¤ì • (ì‹¤ì œ DART ì‚¬ì´íŠ¸ íŒŒë¼ë¯¸í„°)
        form_data = {
            'currentPage': '1',
            'maxResults': '50',
            'maxLinks': '10',
            'sort': 'date',
            'series': 'desc',
            'textCrpNm': company_name,  # íšŒì‚¬ëª…
            'startDate': start_date.strftime('%Y%m%d'),  # YYYYMMDD í˜•ì‹
            'endDate': end_date.strftime('%Y%m%d'),      # YYYYMMDD í˜•ì‹
            'pageGubun': 'corp',
            'decadeType': 'finalReport',
            'recent': '',
            'attachDocNm': ''
        }

        try:
            response = self.session.post(self.search_url, data=form_data)
            response.raise_for_status()
            # response ê²°ê³¼
            print(f"âœ… ìš”ì²­ ì„±ê³µ: ìƒíƒœ ì½”ë“œ {response.status_code}")

            return self.parse_search_results(response.text)

        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    def parse_recent_results(self, html_content):
        """ìµœê·¼ ê³µì‹œ HTML íŒŒì‹±í•˜ì—¬ ê³µì‹œ ëª©ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        disclosures = []

        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table', class_='tbList')
        if not table:
            print("âŒ ê³µì‹œ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ë°ì´í„° í–‰ë“¤ ì°¾ê¸°
        tbody = table.find('tbody')
        if not tbody:
            print("âŒ tbodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        rows = tbody.find_all('tr')
        print(f"ğŸ“Š {len(rows)}ê°œì˜ ê³µì‹œ ë°œê²¬")

        for row in rows:
            try:
                cols = row.find_all('td')
                if len(cols) < 6:  # ìµœê·¼ ê³µì‹œëŠ” 6ê°œ ì»¬ëŸ¼
                    print(f"âš ï¸ ì»¬ëŸ¼ ìˆ˜ ë¶€ì¡±: {len(cols)}ê°œ (ìµœì†Œ 6ê°œ í•„ìš”)")
                    continue

                # ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ ìˆœì„œ: ì‹œê°„, íšŒì‚¬ëª…, ë³´ê³ ì„œëª…, ì œì¶œì¸, ì ‘ìˆ˜ì¼ì, ì‹œì¥êµ¬ë¶„)
                time_str = cols[0].get_text(strip=True)  # ì‹œê°„
                company_cell = cols[1]  # íšŒì‚¬ëª…
                report_cell = cols[2]  # ë³´ê³ ì„œëª…
                submitter = cols[3].get_text(strip=True)  # ì œì¶œì¸
                date = cols[4].get_text(strip=True)  # ì ‘ìˆ˜ì¼ì
                market = cols[5].get_text(strip=True)  # ì‹œì¥êµ¬ë¶„ (ìœ /ì½”/ê³µ)

                # íšŒì‚¬ëª… ì¶”ì¶œ
                company_link = company_cell.find('a')
                company_name = company_link.get_text(strip=True) if company_link else company_cell.get_text(strip=True)

                # ë³´ê³ ì„œëª…ê³¼ ë§í¬ ì¶”ì¶œ
                report_link = report_cell.find('a')
                rcept_no = ""
                if report_link:
                    report_name = report_link.get_text(strip=True)
                    report_url = report_link.get('href', '')

                    # onclickì—ì„œ rcpNo ì¶”ì¶œ
                    onclick = report_link.get('onclick', '')
                    if onclick:
                        # openDartMainAll('20251010800773') í˜•íƒœì—ì„œ ì ‘ìˆ˜ë²ˆí˜¸ ì¶”ì¶œ
                        rcpno_match = re.search(r"'(\d{14})'", onclick)
                        if rcpno_match:
                            rcept_no = rcpno_match.group(1)

                    # hrefì—ì„œë„ rcpNo ì¶”ì¶œ ì‹œë„
                    if not rcept_no and 'rcpNo=' in report_url:
                        rcpno_match = re.search(r'rcpNo=(\d+)', report_url)
                        if rcpno_match:
                            rcept_no = rcpno_match.group(1)
                else:
                    report_name = report_cell.get_text(strip=True)
                    report_url = ""

                # URL ìƒì„±
                if rcept_no and rcept_no.isdigit() and len(rcept_no) == 14:
                    full_url = f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={rcept_no}"
                elif report_url and report_url.startswith('/'):
                    full_url = f"https://dart.fss.or.kr{report_url}"
                else:
                    full_url = report_url if report_url else ""

                # ì •ì •ê³µì‹œì¸ì§€ í™•ì¸
                correction_keywords = ['[ê¸°ì¬ì •ì •]', '[ì²¨ë¶€ì •ì •]', '[ì²¨ë¶€ì¶”ê°€]', '[ì •ì •ëª…ë ¹ë¶€ê³¼]', '[ì •ì •ì œì¶œìš”êµ¬]', 'ì •ì •', 'í’ë¬¸']
                is_correction = any(keyword in report_name for keyword in correction_keywords)

                disclosure = {
                    'number': rcept_no,
                    'company': company_name,
                    'report': report_name,
                    'submitter': submitter,
                    'date': date,
                    'time': time_str,
                    'market': market,
                    'rcept_no': rcept_no,
                    'url': full_url,
                    'is_correction': is_correction
                }

                disclosures.append(disclosure)
                time.sleep(0.1)

            except Exception as e:
                print(f"âš ï¸ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        return disclosures

    def parse_search_results(self, html_content):
        """HTML íŒŒì‹±í•˜ì—¬ ê³µì‹œ ëª©ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')

        disclosures = []

        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table', class_='tbList')
        if not table:
            print("âŒ ê³µì‹œ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ë°ì´í„° í–‰ë“¤ ì°¾ê¸°
        rows = table.find('tbody').find_all('tr')

        for row in rows:
            try:
                cols = row.find_all('td')
                if len(cols) < 5:
                    continue

                # ë°ì´í„° ì¶”ì¶œ
                number = cols[0].get_text(strip=True)
                company_cell = cols[1]
                report_cell = cols[2]
                submitter = cols[3].get_text(strip=True)
                date = cols[4].get_text(strip=True)

                # íšŒì‚¬ëª… ì¶”ì¶œ
                company_link = company_cell.find('a')
                company_name = company_link.get_text(strip=True) if company_link else ""

                # ë³´ê³ ì„œëª…ê³¼ ë§í¬ ì¶”ì¶œ
                report_link = report_cell.find('a')
                if report_link:
                    report_name = report_link.get_text(strip=True)
                    report_url = report_link.get('href', '')

                    # rcpNo ì¶”ì¶œ
                    rcept_no_match = re.search(r'rcpNo=(\d+)', report_url)
                    rcept_no = rcept_no_match.group(1) if rcept_no_match else ""
                else:
                    report_name = report_cell.get_text(strip=True)
                    report_url = ""
                    rcept_no = ""

                # ì •ì •ê³µì‹œì¸ì§€ í™•ì¸
                correction_keywords = ['[ê¸°ì¬ì •ì •]', '[ì²¨ë¶€ì •ì •]', '[ì²¨ë¶€ì¶”ê°€]', '[ì •ì •ëª…ë ¹ë¶€ê³¼]', '[ì •ì •ì œì¶œìš”êµ¬]', 'ì •ì •', 'í’ë¬¸']
                is_correction = any(keyword in report_name for keyword in correction_keywords)

                time.sleep(0.2)
                # if is_correction == 'false' or is_correction == False or is_correction == 'False':
                #     continue

                disclosure = {
                    'number': number,
                    'company': company_name,
                    'report': report_name,
                    'submitter': submitter,
                    'date': date,
                    'rcept_no': rcept_no,
                    'url': f"https://dart.fss.or.kr{report_url}" if report_url else "",
                    'is_correction': is_correction
                }

                disclosures.append(disclosure)

            except Exception as e:
                print(f"âš ï¸ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        return disclosures

    def find_corrections(self, company_name, start_date=None, end_date=None, days=365):
        """ì •ì •ê³µì‹œë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜"""
        all_disclosures = self.search_company_disclosures(company_name, start_date, end_date, days)

        corrections = [d for d in all_disclosures if d['is_correction']]

        print(f"ğŸ“Š ì „ì²´ ê³µì‹œ: {len(all_disclosures)}ê±´")
        print(f"ğŸ”§ ì •ì •ê³µì‹œ: {len(corrections)}ê±´")

        return corrections

    def print_results(self, corrections):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\nâœ… {len(corrections)}ê±´ì˜ ì •ì •ê³µì‹œ ë°œê²¬!")
        print("=" * 80)

        if not corrections:
            print("ì •ì •ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i, item in enumerate(corrections, 1):
            print(f"{i}. ğŸ“‹ {item['company']}")
            print(f"   ë³´ê³ ì„œ: {item['report']}")
            print(f"   ì œì¶œì¸: {item['submitter']}")
            print(f"   ë‚ ì§œ: {item['date']}")
            if item['url']:
                print(f"   ğŸ”— ë§í¬: {item['url']}")
            print("-" * 80)


if __name__ == "__main__":
    crawler = DartWebCrawler()

    print("ğŸ¯ DART ì›¹ í¬ë¡¤ë§ ì •ì •ê³µì‹œ ì°¾ê¸°")
    print("=" * 35)

    while True:
        company = input("\nğŸ¢ íšŒì‚¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()

        if company.lower() == 'q':
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not company:
            print("âŒ íšŒì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        # ë‚ ì§œ ì…ë ¥ (ì„ íƒì‚¬í•­)
        print("\nğŸ“… ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • (ì„ íƒì‚¬í•­):")
        start_input = input("ì‹œì‘ì¼ (YYYY-MM-DD ë˜ëŠ” ì—”í„°ë¡œ ê¸°ë³¸ê°’): ").strip()
        end_input = input("ì¢…ë£Œì¼ (YYYY-MM-DD ë˜ëŠ” ì—”í„°ë¡œ ì˜¤ëŠ˜): ").strip()

        start_date = start_input if start_input else None
        end_date = end_input if end_input else None

        if not start_date and not end_date:
            days_input = input("ìµœê·¼ ë©°ì¹ ê°„ ê²€ìƒ‰? (ê¸°ë³¸ê°’: 365ì¼): ").strip()
            days = int(days_input) if days_input.isdigit() else 365
        else:
            days = 365

        # ê²€ìƒ‰ ì‹¤í–‰
        try:
            corrections = crawler.find_corrections(company, start_date, end_date, days)
            crawler.print_results(corrections)
        except ValueError as e:
            print(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
            print("ì˜¬ë°”ë¥¸ í˜•ì‹: YYYY-MM-DD (ì˜ˆ: 2025-01-01)")